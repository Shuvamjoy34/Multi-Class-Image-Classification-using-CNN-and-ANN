{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " VGG 16 of CNN car vs bike vs random Image Classification_ with default size_ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shuvamjoy34/Multi-Class-Image-Classification-using-CNN-and-ANN/blob/main/VGG_16_of_CNN_car_vs_bike_vs_random_Image_Classification__with_default_size.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anIWn3g4IiBr"
      },
      "source": [
        "# **Convolutional Neural Network Based Car vs Bike vs Random Images Multi Class Classification**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHQrsv4mJQSW"
      },
      "source": [
        "**Importing required python packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IwzPSB_5BdE"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from keras.utils.data_utils import Sequence\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.keras import balanced_batch_generator\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras \n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cv2 import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from numpy import array \n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "#sess = tf.Session(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49hqPMWwL2gC"
      },
      "source": [
        "**Connecting google colab notebook to my google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws4O_ErfJ2Dh",
        "outputId": "a8293f4c-bfb5-411d-9956-b3e1ff96f61e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4vi8xaKMvbj"
      },
      "source": [
        "**Importing the image files with JPG format**\n",
        "\n",
        "\n",
        "**Resizing and Labelling the images**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvMbsJ3t5Pec"
      },
      "source": [
        "data=[]\n",
        "labels=[]\n",
        "car=os.listdir(\"/content/drive/My Drive/Images/car/\")\n",
        "for cars in car:\n",
        "    try:\n",
        "        image=cv2.imread(\"/content/drive/My Drive/Images/car/\"+ cars)\n",
        "        image_from_array = Image.fromarray(image, 'RGB')\n",
        "        size_image = image_from_array.resize((224,224))\n",
        "        data.append(np.array(size_image))\n",
        "        labels.append(0)\n",
        "    except AttributeError:\n",
        "        print(\"\")\n",
        "\n",
        "bike=os.listdir(\"/content/drive/My Drive/Images/bike/\")\n",
        "for bikes in bike:\n",
        "    try:\n",
        "        image=cv2.imread(\"/content/drive/My Drive/Images/bike/\"+ bikes)\n",
        "        image_from_array = Image.fromarray(image, 'RGB')\n",
        "        size_image = image_from_array.resize((224,224))\n",
        "        data.append(np.array(size_image))\n",
        "        labels.append(1)\n",
        "    except AttributeError:\n",
        "        print(\"\")\n",
        "random=os.listdir(\"/content/drive/My Drive/Images/random/\")\n",
        "for randoms in random:\n",
        "    try:\n",
        "        image=cv2.imread(\"/content/drive/My Drive/Images/random/\"+ randoms)\n",
        "        image_from_array = Image.fromarray(image, 'RGB')\n",
        "        size_image = image_from_array.resize((224,224))\n",
        "        data.append(np.array(size_image))\n",
        "        labels.append(2)\n",
        "    except AttributeError:\n",
        "        print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5VbX_iG5tDU"
      },
      "source": [
        "images=np.array(data)\n",
        "labels=np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpIgw9cX7Brg"
      },
      "source": [
        "s=np.arange(images.shape[0])\n",
        "np.random.shuffle(s)\n",
        "images=images[s]\n",
        "labels=labels[s]\n",
        "num_classes=len(np.unique(labels))\n",
        "len_data=len(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ul0FTN2OW71"
      },
      "source": [
        "**Diving the image dataset into training, validation and testing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KuERdWF7C2h"
      },
      "source": [
        "images =images.astype(np.float32)\n",
        "images = images/255\n",
        "\n",
        "train_x , x , train_y , y = train_test_split(images , labels , \n",
        "                                            test_size = 0.2 ,\n",
        "                                            random_state = 111)\n",
        "\n",
        "eval_x , test_x , eval_y , test_y = train_test_split(x , y , \n",
        "                                                    test_size = 0.5 , \n",
        "                                                    random_state = 111)\n",
        "\n",
        "plt.figure(1 , figsize = (15 ,5))\n",
        "n = 0 \n",
        "for z , j in zip([train_y , eval_y , test_y] , ['train labels','eval labels','test labels']):\n",
        "    n += 1\n",
        "    plt.subplot(1 , 3  , n)\n",
        "    sns.countplot(x = z )\n",
        "    plt.title(j)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7FXYtXDRP6d"
      },
      "source": [
        "#x=x.reshape(x.shape[0], -1)\n",
        "#y=y.reshape(y.shape[0], -1)\n",
        "\n",
        "#train_x=train_x.reshape(train_x.shape[0], -1)\n",
        "#train_y=train_y.reshape(train_y.shape[0], -1)\n",
        "#eval_x=eval_x.reshape(eval_x.shape[0], -1)\n",
        "#eval_y=eval_y.reshape(eval_y.shape[0], -1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4WA6XozR4RF",
        "outputId": "6a127d2a-57c9-4d55-c39b-3d4bd6ae9f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "#Over-sampling: SMOTE\n",
        "#SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, \n",
        "#based on those that already exist. It works randomly picking a point from the minority class and computing \n",
        "#the k-nearest neighbors for this point.The synthetic points are added between the chosen point and its neighbors.\n",
        "#We'll use ratio='minority' to resample the minority class.\n",
        "#smote = SMOTE('minority',kind='regular',k_neighbors=2)\n",
        "\n",
        "#train_x_sm, train_y_sm = smote.fit_sample(train_x,train_y)\n",
        "print(train_x_sm.shape, train_y_sm.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(107, 3072) (107,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GuPx6wdTv3Y"
      },
      "source": [
        "**Encoding the labels of the target class of training,validation and testing dataset from(0,1,2) to (0,1) where 1 pops up when there is a class in the array else 0 pops up**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqGsbD8a_DsY"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "train_y = lb.fit_transform(train_y)\n",
        "test_y = lb.transform(test_y)\n",
        "eval_y = lb.transform(eval_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QuZ9pYQLquy"
      },
      "source": [
        "from tensorflow.python.keras import regularizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "#kernel_regularizer=regularizers.l2(0.05)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACWg91qrU_Qf"
      },
      "source": [
        "**Creating a basic Convolutional Neural Network architecture and using Random Search and Grid Search to find the best Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH-ZuqHW2L3x"
      },
      "source": [
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLW_q-uwcAfQ"
      },
      "source": [
        "#vgg16\n",
        "vggmodel = Sequential()\n",
        "vggmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "#keras.layers.BatchNormalization()\n",
        "vggmodel.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vggmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "#keras.layers.BatchNormalization()\n",
        "vggmodel.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vggmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "#keras.layers.BatchNormalization()\n",
        "vggmodel.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vggmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "#keras.layers.BatchNormalization()\n",
        "vggmodel.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vggmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vggmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "#keras.layers.BatchNormalization()\n",
        "vggmodel.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Oa35rl2Q5M"
      },
      "source": [
        "keras.layers.Dropout(0.5)\n",
        "vggmodel.add(Flatten())\n",
        "vggmodel.add(Dense(units=4096,activation=\"relu\"))\n",
        "keras.layers.Dropout(0.5)\n",
        "vggmodel.add(Dense(units=4096,activation=\"relu\"))\n",
        "keras.layers.Dropout(0.5)\n",
        "vggmodel.add(Dense(units=3, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj3OkIly2xbF",
        "outputId": "6824762d-e358-4eb1-da3c-616f70e11573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        }
      },
      "source": [
        "vggmodel.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 134,272,835\n",
            "Trainable params: 134,272,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-97kUBD_ZfaH"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "#opt = Adam(lr=1e-3, decay=1e-3 / 30)\n",
        "vggmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGczUBGS24ft"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "#vggmodel.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "VGG=vggmodel.fit(train_x,train_y,batch_size = 32,epochs=100,verbose=1,validation_data=(eval_x, eval_y),callbacks=[checkpoint,early])\n",
        "VGG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3VfSOlu5Fd_",
        "outputId": "e41c9453-7354-439d-998e-5288e43ccc00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Once the model is trained we can evaluate it on Test data.\n",
        "\n",
        "# Evaluating the model \n",
        "vggscore = vggmodel.evaluate(test_x, test_y, verbose=0)\n",
        "print('Test Loss:', vggscore[0])\n",
        "print('Test accuracy:', vggscore[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.0781618356704712\n",
            "Test accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xb0R3Ul5OzS",
        "outputId": "1858d806-7cef-43ae-9cdf-295ceb2aeab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "Y_pred = vggmodel.predict(test_x)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "target_names = ['class 0(car)', 'class 1(bike)','class 2(random)']\n",
        "                                               \n",
        "print(classification_report(np.argmax(test_y,axis=1), y_pred,target_names=target_names))\n",
        "\n",
        "print(confusion_matrix(np.argmax(test_y,axis=1), y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "   class 0(car)       0.00      0.00      0.00         4\n",
            "  class 1(bike)       0.00      0.00      0.00         1\n",
            "class 2(random)       0.50      1.00      0.67         5\n",
            "\n",
            "       accuracy                           0.50        10\n",
            "      macro avg       0.17      0.33      0.22        10\n",
            "   weighted avg       0.25      0.50      0.33        10\n",
            "\n",
            "[[0 0 4]\n",
            " [0 0 1]\n",
            " [0 0 5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEu6Rpr-BrvZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(vggmodel.history.history[\"accuracy\"])\n",
        "plt.plot(vggmodel.history.history['val_accuracy'])\n",
        "plt.plot(vggmodel.history.history['loss'])\n",
        "plt.plot(vggmodel.history.history['val_loss'])\n",
        "plt.title(\"model accuracy & loss comparison\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}